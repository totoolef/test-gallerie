"""
Module de g√©n√©ration de l√©gendes automatiques pour les images avec BLIP.
"""

import os
# Fix pour OpenMP sur macOS - DOIT √™tre au tout d√©but
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch
# Fix pour √©viter les probl√®mes de threading avec FAISS/OpenMP
torch.set_num_threads(1)

from typing import Optional
from PIL import Image
import warnings

# D√©sactiver les warnings pour √©viter le bruit
warnings.filterwarnings("ignore", category=UserWarning)

# Import transformers APR√àS torch
from transformers import BlipProcessor, BlipForConditionalGeneration


# Singleton global pour le captioner
_captioner_instance = None


class BLIPCaptioner:
    """Classe pour g√©n√©rer des l√©gendes automatiques avec BLIP."""
    
    def __init__(self, model_name: str = "Salesforce/blip-image-captioning-base", device: str = None):
        """
        Initialise le mod√®le BLIP pour la g√©n√©ration de l√©gendes.
        
        Args:
            model_name: Nom du mod√®le BLIP √† utiliser
            device: Device √† utiliser ('cuda', 'cpu', ou None pour auto-d√©tection)
        """
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.device = device
        self.model_name = model_name
        self._processor = None
        self._model = None
        
    def _load_model(self):
        """Charge le mod√®le BLIP de mani√®re lazy (seulement quand n√©cessaire)."""
        if self._model is None:
            print(f"üì¶ Chargement du mod√®le BLIP: {self.model_name}")
            print(f"üîß Device: {self.device}")
            
            try:
                self._processor = BlipProcessor.from_pretrained(self.model_name)
                self._model = BlipForConditionalGeneration.from_pretrained(
                    self.model_name,
                    dtype=torch.float32,
                    low_cpu_mem_usage=True
                ).to(self.device)
                self._model.eval()
                
                # D√©sactiver le gradient pour √©viter les probl√®mes
                for param in self._model.parameters():
                    param.requires_grad = False
                    
                print(f"‚úÖ Mod√®le BLIP charg√© avec succ√®s")
                    
            except Exception as e:
                raise RuntimeError(f"‚ùå Erreur lors du chargement du mod√®le BLIP: {e}")
    
    def generate_caption(self, image: Image.Image, timeout: float = 15.0) -> str:
        """
        G√©n√®re une l√©gende automatique pour une image.
        
        Args:
            image: Image PIL √† d√©crire
            timeout: Timeout en secondes (d√©faut: 5.0)
            
        Returns:
            L√©gende g√©n√©r√©e (texte) ou "unknown" si erreur/timeout
        """
        # Charger le mod√®le si n√©cessaire
        self._load_model()
        
        try:
            # S'assurer que l'image est valide
            if image is None or image.size[0] == 0 or image.size[1] == 0:
                return "unknown"
            
            # Convertir en RGB si n√©cessaire
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # G√©n√©rer la l√©gende avec gestion de timeout
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("Timeout lors de la g√©n√©ration de la l√©gende")
            
            # Sur macOS/Linux, utiliser signal pour timeout
            old_handler = None
            try:
                if hasattr(signal, 'SIGALRM'):
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(int(timeout))
                
                with torch.inference_mode():
                    inputs = self._processor(image, return_tensors="pt").to(self.device)
                    # Augmenter l√©g√®rement les param√®tres pour de meilleures l√©gendes sur CPU
                    out = self._model.generate(**inputs, max_length=60, min_length=5, num_beams=4, no_repeat_ngram_size=2)
                    caption = self._processor.decode(out[0], skip_special_tokens=True)
                    
                    result = caption.strip()
                    
                    # Annuler l'alarme si succ√®s
                    if hasattr(signal, 'SIGALRM'):
                        signal.alarm(0)
                    
                    # Si BLIP renvoie quelque chose de non vide et valide, on le garde
                    if result and result.strip() and len(result.strip()) >= 3:
                        # V√©rifier que ce n'est pas juste "unknown" ou des mots vides
                        result_lower = result.strip().lower()
                        if result_lower not in ['unknown', 'error', '']:
                            return result.strip()
                    return "unknown"
            except (TimeoutError, Exception) as e:
                if hasattr(signal, 'SIGALRM'):
                    signal.alarm(0)
                return "unknown"
                
        except Exception as e:
            # En cas d'erreur, retourner "unknown"
            return "unknown"


def get_captioner(model_name: str = "Salesforce/blip-image-captioning-base", device: str = None) -> BLIPCaptioner:
    """
    Factory function pour obtenir un BLIPCaptioner (singleton).
    
    Args:
        model_name: Nom du mod√®le BLIP
        device: Device √† utiliser
    
    Returns:
        Instance de BLIPCaptioner (singleton)
    """
    global _captioner_instance
    if _captioner_instance is None:
        _captioner_instance = BLIPCaptioner(model_name=model_name, device=device)
    return _captioner_instance

