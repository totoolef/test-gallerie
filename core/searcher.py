"""
Module de recherche pour rechercher dans les mÃ©dias indexÃ©s via une requÃªte texte.
"""

import os
# Fix pour OpenMP sur macOS - DOIT Ãªtre au tout dÃ©but
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch
# Fix pour Ã©viter les problÃ¨mes de threading avec FAISS/OpenMP
torch.set_num_threads(1)

import json
import faiss
import numpy as np
from typing import List, Dict, Tuple, Optional

from .clip_utils import CLIPEmbedder
from .reranker import CrossEncoderReranker, get_reranker, rerank_results
from .filters import filter_metadata


def load_index_and_metadata(index_path: str = "index.faiss", 
                            metadata_path: str = "metadata.json") -> Tuple[faiss.Index, List[Dict]]:
    """
    Charge l'index FAISS et les mÃ©tadonnÃ©es.
    
    Args:
        index_path: Chemin vers le fichier d'index FAISS
        metadata_path: Chemin vers le fichier de mÃ©tadonnÃ©es JSON
        
    Returns:
        Tuple (index FAISS, liste de mÃ©tadonnÃ©es)
    """
    if not os.path.exists(index_path):
        print("\n" + "="*80)
        print("âŒ ERREUR: L'index FAISS n'existe pas encore")
        print("="*80)
        print(f"\nğŸ“ Fichier manquant: {os.path.abspath(index_path)}")
        print("\nğŸ’¡ SOLUTION:")
        print("   1. CrÃ©ez un dossier 'data/' contenant vos images et vidÃ©os")
        print("   2. ExÃ©cutez: python extract_embeddings.py")
        print("   3. Puis rÃ©essayez votre recherche")
        print("\n" + "="*80)
        raise FileNotFoundError(f"L'index {index_path} n'existe pas. ExÃ©cutez d'abord extract_embeddings.py")
    
    if not os.path.exists(metadata_path):
        print("\n" + "="*80)
        print("âŒ ERREUR: Les mÃ©tadonnÃ©es n'existent pas encore")
        print("="*80)
        print(f"\nğŸ“ Fichier manquant: {os.path.abspath(metadata_path)}")
        print("\nğŸ’¡ SOLUTION:")
        print("   1. CrÃ©ez un dossier 'data/' contenant vos images et vidÃ©os")
        print("   2. ExÃ©cutez: python extract_embeddings.py")
        print("   3. Puis rÃ©essayez votre recherche")
        print("\n" + "="*80)
        raise FileNotFoundError(f"Les mÃ©tadonnÃ©es {metadata_path} n'existent pas. ExÃ©cutez d'abord extract_embeddings.py")
    
    print(f"ğŸ“‚ Chargement de l'index: {index_path}")
    try:
        index = faiss.read_index(index_path)
    except Exception as e:
        print(f"âŒ Erreur lors du chargement de l'index: {e}")
        raise
    
    print(f"ğŸ“‚ Chargement des mÃ©tadonnÃ©es: {metadata_path}")
    try:
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
    except Exception as e:
        print(f"âŒ Erreur lors du chargement des mÃ©tadonnÃ©es: {e}")
        raise
    
    print(f"âœ… Index chargÃ©: {index.ntotal} embedding(s)")
    return index, metadata


def translate_fr2en(query_fr: str) -> str:
    """
    Traduit une requÃªte franÃ§aise en anglais (implÃ©mentation locale avec transformers).
    
    Args:
        query_fr: RequÃªte en franÃ§ais
        
    Returns:
        RequÃªte traduite en anglais
    """
    try:
        from transformers import MarianMTModel, MarianTokenizer
        
        # ModÃ¨le Helsinki-NLP FRâ†’EN
        model_name = "Helsinki-NLP/opus-mt-fr-en"
        
        # Charger le modÃ¨le et tokenizer (lazy loading avec cache)
        tokenizer = MarianTokenizer.from_pretrained(model_name)
        model = MarianMTModel.from_pretrained(model_name)
        
        # Traduire
        with torch.inference_mode():
            inputs = tokenizer(query_fr, return_tensors="pt", padding=True, truncation=True)
            translated = model.generate(**inputs, max_length=512)
            translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)
        
        return translated_text.strip()
        
    except Exception as e:
        # En cas d'erreur, retourner la requÃªte originale
        print(f"âš ï¸  Erreur lors de la traduction: {e}")
        return query_fr


def expand_query(query_fr: str, enable_fr: bool = True, enable_en: bool = True, auto_translate: bool = False) -> List[str]:
    """
    Enrichit la requÃªte avec des variations bilingues (FR/EN) pour amÃ©liorer la prÃ©cision.
    
    Args:
        query_fr: RequÃªte texte originale (franÃ§ais)
        enable_fr: Si True, gÃ©nÃ¨re des variantes franÃ§aises
        enable_en: Si True, gÃ©nÃ¨re des variantes anglaises
        auto_translate: Si True, traduit automatiquement la requÃªte en anglais
        
    Returns:
        Liste de variantes de la requÃªte
    """
    variations = []
    
    # Variantes franÃ§aises
    if enable_fr:
        variations.append(query_fr)
        variations.append(f"photo de {query_fr}")
        variations.append(f"image de {query_fr}")
        variations.append(f"vue d'ensemble de {query_fr}")
        variations.append(f"gros plan de {query_fr}")
        variations.append(f"plan large de {query_fr}")
    
    # Variantes anglaises (si traduction activÃ©e)
    if enable_en and auto_translate:
        try:
            query_en = translate_fr2en(query_fr)
            if query_en and query_en != query_fr:
                variations.append(query_en)
                variations.append(f"a photo of {query_en}")
                variations.append(f"an image showing {query_en}")
                variations.append(f"a close-up of {query_en}")
                variations.append(f"a wide shot of {query_en}")
        except Exception as e:
            print(f"âš ï¸  Erreur lors de la traduction pour query expansion: {e}")
    
    # Si aucune variation n'a Ã©tÃ© gÃ©nÃ©rÃ©e, retourner au moins la requÃªte originale
    if not variations:
        variations.append(query_fr)
    
    return variations


def compute_dynamic_threshold(cosine_scores: np.ndarray, 
                              k: int = 10, 
                              min_floor: float = 0.25, 
                              max_ceil: float = 0.45) -> float:
    """
    Calcule un seuil dynamique basÃ© sur la distribution des scores.
    
    Args:
        cosine_scores: Array numpy des scores de similaritÃ© cosinus
        k: Nombre de meilleurs scores Ã  considÃ©rer (dÃ©faut: 10)
        min_floor: Seuil minimum (dÃ©faut: 0.25)
        max_ceil: Seuil maximum (dÃ©faut: 0.45)
        
    Returns:
        Seuil dynamique calculÃ© (clampÃ© entre min_floor et max_ceil)
    """
    if len(cosine_scores) == 0:
        return min_floor
    
    # Prendre les k meilleurs scores (ou tous si moins de k)
    top_k_scores = np.sort(cosine_scores)[::-1][:min(k, len(cosine_scores))]
    
    if len(top_k_scores) == 0:
        return min_floor
    
    # Calculer moyenne et Ã©cart-type
    mean_score = np.mean(top_k_scores)
    std_score = np.std(top_k_scores)
    
    # Seuil = mean - std (on veut Ãªtre un peu en dessous de la moyenne)
    threshold = mean_score - std_score
    
    # Clamper entre min_floor et max_ceil
    threshold = np.clip(threshold, min_floor, max_ceil)
    
    return float(threshold)


def search(query_text: str, 
           index: faiss.Index,
           metadata: List[Dict],
           embedder: CLIPEmbedder,
           top_k: int = 5,
           use_query_expansion: bool = True,
           auto_translate: bool = False,
           use_dynamic_threshold: bool = False,
           fixed_threshold: float = 0.3,
           always_rerank: bool = False,
           rerank_if_below: Optional[float] = None,
           reranker: Optional[CrossEncoderReranker] = None,
           use_captions: bool = True,
           filtered_indices: Optional[List[int]] = None,
           media_type: Optional[str] = None,
           date_range: Optional[Tuple] = None,
           include_dirs: Optional[List[str]] = None) -> List[Dict]:
    """
    Recherche les mÃ©dias les plus pertinents pour une requÃªte texte.
    
    Args:
        query_text: RequÃªte texte (franÃ§ais)
        index: Index FAISS
        metadata: Liste des mÃ©tadonnÃ©es
        embedder: Instance de CLIPEmbedder
        top_k: Nombre de rÃ©sultats Ã  retourner
        use_query_expansion: Si True, utilise la query expansion bilingue
        auto_translate: Si True, traduit automatiquement la requÃªte en anglais pour expansion
        use_dynamic_threshold: Si True, utilise un seuil dynamique
        fixed_threshold: Seuil fixe si use_dynamic_threshold=False
        always_rerank: Si True, applique toujours le rerank
        rerank_if_below: Si non-None et best_cosine < rerank_if_below, applique rerank
        reranker: Instance de CrossEncoderReranker (optionnel)
        use_captions: Si True, utilise les captions pour le rerank
        filtered_indices: Liste d'indices Ã  considÃ©rer (pour filtres prÃ©-FAISS)
        media_type: Type de mÃ©dia Ã  filtrer ('image', 'video', ou None)
        date_range: Tuple (date_debut, date_fin) pour filtrer par date
        include_dirs: Liste de dossiers Ã  inclure
        
    Returns:
        Liste de dictionnaires avec "path", "score", "cosine_score", "meta"
    """
    print(f"ğŸ” Recherche: \"{query_text}\"")
    
    # Encoder la requÃªte texte (avec ou sans expansion)
    print("ğŸ“ Encodage de la requÃªte...")
    try:
        if use_query_expansion:
            # GÃ©nÃ©rer des variantes bilingues de la requÃªte
            query_variations = expand_query(
                query_fr=query_text,
                enable_fr=True,
                enable_en=auto_translate,
                auto_translate=auto_translate
            )
            print(f"   ğŸ”„ GÃ©nÃ©ration de {len(query_variations)} variantes de la requÃªte...")
            
            # Encoder chaque variante
            query_embeddings = []
            for variation in query_variations:
                try:
                    embedding = embedder.encode_text(variation)
                    query_embeddings.append(embedding)
                except Exception as e:
                    # Si une variante Ã©choue, continuer avec les autres
                    continue
            
            if not query_embeddings:
                # Fallback : utiliser la requÃªte originale
                query_embedding = embedder.encode_text(query_text)
            else:
                # Moyenne des embeddings des variantes (agrÃ©gation)
                query_embeddings_array = np.array(query_embeddings)
                mean_embedding = np.mean(query_embeddings_array, axis=0)
                
                # Re-normaliser (important pour cosine similarity)
                norm = np.linalg.norm(mean_embedding)
                if norm > 0:
                    mean_embedding = mean_embedding / norm
                
                query_embedding = mean_embedding
        else:
            # Encoder uniquement la requÃªte originale
            query_embedding = embedder.encode_text(query_text)
        
        # Nettoyage : s'assurer que c'est bien float32 et bien reshapÃ©
        query_embedding = query_embedding.astype('float32')
        query_embedding = query_embedding.reshape(1, -1)
        
        # VÃ©rifier qu'il n'y a pas de NaN ou Inf
        if not np.all(np.isfinite(query_embedding)):
            raise ValueError("L'embedding de la requÃªte contient des valeurs invalides")
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'encodage de la requÃªte: {e}")
        raise
    
    # Normaliser la requÃªte pour cosine similarity (si nÃ©cessaire)
    faiss.normalize_L2(query_embedding)
    
    # Appliquer les filtres si disponibles
    if filtered_indices is None:
        # Si filtered_indices n'est pas fourni, calculer depuis les filtres
        if media_type is not None or date_range is not None or include_dirs is not None:
            filtered_indices = filter_metadata(
                metadata=metadata,
                media_type=media_type,
                date_range=date_range,
                include_dirs=include_dirs
            )
            print(f"ğŸ“Š Filtres appliquÃ©s: {len(filtered_indices)}/{len(metadata)} indices valides")
    
    # Rechercher dans l'index
    # Si rerank activÃ©, chercher plus de candidats
    search_k = top_k * 3 if (always_rerank or rerank_if_below is not None) else top_k
    print(f"ğŸ” Recherche des {search_k} rÃ©sultats les plus pertinents...")
    
    try:
        # Si filtres actifs avec beaucoup d'indices valides, on peut chercher plus
        # Pour l'instant, on fait post-filtrage
        distances, indices = index.search(query_embedding, min(search_k * 2, index.ntotal))
    except Exception as e:
        print(f"âŒ Erreur lors de la recherche dans l'index: {e}")
        raise
    
    # Construire les rÃ©sultats
    candidates = []
    cosine_scores = []
    
    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
        # Filtrer les valeurs invalides (NaN, Inf, ou indices hors limites)
        if not np.isfinite(distance) or distance < 0:
            continue
        if idx < 0 or idx >= len(metadata):
            continue
        
        # Appliquer filtres prÃ©-FAISS si disponibles
        if filtered_indices is not None and idx not in filtered_indices:
            continue
        
        meta = metadata[idx]
        file_path = meta.get("file_path", "")
        cosine_score = float(distance)
        
        candidates.append({
            "path": file_path,
            "score": cosine_score,
            "meta": meta
        })
        cosine_scores.append(cosine_score)
    
    # Calculer le seuil (dynamique ou fixe)
    if use_dynamic_threshold and len(cosine_scores) > 0:
        threshold = compute_dynamic_threshold(np.array(cosine_scores))
        print(f"ğŸ“Š Seuil dynamique calculÃ©: {threshold:.4f}")
        # Si le seuil dynamique est trop Ã©levÃ©, utiliser un minimum plus bas
        if threshold > 0.25:
            threshold = max(0.12, threshold * 0.6)  # RÃ©duire de 40% mais minimum 0.12
            print(f"   ğŸ”§ Seuil ajustÃ© Ã : {threshold:.4f} (trop Ã©levÃ©)")
    else:
        threshold = fixed_threshold
        print(f"ğŸ“Š Seuil fixe utilisÃ©: {threshold:.4f}")
    
    # Afficher les scores pour debug
    if len(cosine_scores) > 0:
        top_scores = sorted(cosine_scores, reverse=True)[:5]
        print(f"ğŸ“Š Top 5 scores bruts: {[f'{s:.4f}' for s in top_scores]}")
    
    # Filtrer par seuil
    filtered_candidates = [c for c in candidates if c["score"] >= threshold]
    
    # Si aucun rÃ©sultat aprÃ¨s filtrage, prendre au moins le top 5 mÃªme si sous le seuil
    if not filtered_candidates and len(candidates) > 0:
        print(f"âš ï¸  Aucun rÃ©sultat au-dessus du seuil {threshold:.4f}, affichage du top 5 quand mÃªme")
        filtered_candidates = sorted(candidates, key=lambda x: x["score"], reverse=True)[:5]
    
    # DÃ©terminer si on doit appliquer le rerank
    should_rerank = False
    if always_rerank:
        should_rerank = True
        print("ğŸ”„ Rerank forcÃ© (always_rerank=True)")
    elif rerank_if_below is not None and len(filtered_candidates) > 0:
        best_cosine = max(c["score"] for c in filtered_candidates)
        if best_cosine < rerank_if_below:
            should_rerank = True
            print(f"ğŸ”„ Rerank activÃ© (best_cosine={best_cosine:.4f} < rerank_if_below={rerank_if_below:.4f})")
    
    # Appliquer le rerank si nÃ©cessaire
    if should_rerank and len(filtered_candidates) > 0:
        print("ğŸ”„ Re-ranking des rÃ©sultats avec Cross-Encoder...")
        try:
            if reranker is None:
                reranker = get_reranker()
            
            # Re-scorer les top candidats
            top_rerank = min(10, len(filtered_candidates))
            reranked_results = rerank_results(
                query_text=query_text,
                candidates=filtered_candidates[:top_rerank],
                top_rerank=top_rerank,
                use_captions=use_captions,
                reranker=reranker
            )
            
            # Convertir les rÃ©sultats rerankÃ©s en format Dict avec cosine_score
            results = []
            for r in reranked_results:
                results.append({
                    "path": r["path"],
                    "score": r["score"],  # Score cross-encoder
                    "cosine_score": r.get("cosine_score", 0.0),  # Score FAISS original
                    "meta": r["meta"]
                })
            
            # Trier par score cross-encoder dÃ©croissant
            results.sort(key=lambda x: x["score"], reverse=True)
            
            # Limiter aux top_k
            results = results[:top_k]
            
            print("âœ… Re-ranking terminÃ©")
        except Exception as e:
            print(f"âš ï¸  Erreur lors du re-ranking: {e}")
            print("   Continuation avec les rÃ©sultats FAISS originaux")
            # Garder les rÃ©sultats FAISS originaux
            results = filtered_candidates[:top_k]
    else:
        # Pas de rerank, utiliser les rÃ©sultats FAISS directement
        results = filtered_candidates[:top_k]
        # Ajouter cosine_score pour compatibilitÃ©
        for r in results:
            r["cosine_score"] = r["score"]
    
    return results


def display_results(results: List[Dict]):
    """
    Affiche les rÃ©sultats de recherche de maniÃ¨re formatÃ©e.
    Pour les vidÃ©os, regroupe les frames par fichier et n'affiche chaque vidÃ©o qu'une seule fois
    (avec le meilleur score).
    
    Args:
        results: Liste de dictionnaires avec "path", "score", "cosine_score", "meta"
    """
    if not results:
        print("âŒ Aucun rÃ©sultat trouvÃ©.")
        return
    
    # Grouper les rÃ©sultats par fichier (pour Ã©viter les doublons de vidÃ©os)
    unique_results = {}
    
    for result in results:
        file_path = result.get("path", "")
        score = result.get("score", 0.0)  # Score principal (rerank ou cosine)
        meta = result.get("meta", {})
        
        # Pour les vidÃ©os, on groupe par fichier et on garde le meilleur score
        # Pour les images, on garde chaque rÃ©sultat unique
        if meta.get('media_type') == 'video':
            # Si cette vidÃ©o n'a pas encore Ã©tÃ© vue, ou si ce score est meilleur
            if file_path not in unique_results or score > unique_results[file_path]["score"]:
                unique_results[file_path] = result
        else:
            # Pour les images, on peut avoir le mÃªme fichier plusieurs fois (avec multi-scale)
            # On garde le meilleur score pour chaque fichier
            if file_path not in unique_results or score > unique_results[file_path]["score"]:
                unique_results[file_path] = result
    
    # Convertir en liste et trier par score dÃ©croissant
    unique_results_list = list(unique_results.values())
    unique_results_list.sort(key=lambda x: x.get("score", 0.0), reverse=True)
    
    print("\n" + "="*80)
    print("ğŸ“Š RÃ‰SULTATS DE LA RECHERCHE")
    print("="*80)
    
    for i, result in enumerate(unique_results_list, 1):
        file_path = result.get("path", "")
        score = result.get("score", 0.0)
        cosine_score = result.get("cosine_score", score)
        meta = result.get("meta", {})
        
        print(f"\n{i}. {os.path.basename(file_path)}")
        print(f"   ğŸ“ Chemin: {file_path}")
        # Afficher les deux scores si rerank activÃ©
        if abs(score - cosine_score) > 0.001:
            print(f"   ğŸ“Š Score Cross-Encoder: {score:.4f} | Score FAISS: {cosine_score:.4f}")
        else:
            print(f"   ğŸ“Š Score de similaritÃ©: {score:.4f} (cosine similarity)")
        print(f"   ğŸ¬ Type: {meta.get('media_type', 'unknown')}")
        if meta.get('media_type') == 'video':
            print(f"   ğŸï¸  VidÃ©o (meilleure frame sÃ©lectionnÃ©e)")
    
    print("\n" + "="*80)


def search_by_text(query_text: str,
                   index_path: str = "index.faiss",
                   metadata_path: str = "metadata.json",
                   top_k: int = 5,
                   model_name: str = "openai/clip-vit-large-patch14",
                   embedder: CLIPEmbedder = None,
                   use_query_expansion: bool = True,
                   auto_translate: bool = False,
                   use_dynamic_threshold: bool = False,
                   fixed_threshold: float = 0.3,
                   always_rerank: bool = False,
                   rerank_if_below: Optional[float] = None,
                   use_reranking: bool = True,
                   use_captions: bool = True) -> List[Dict]:
    """
    Fonction principale pour rechercher dans les mÃ©dias.
    
    Args:
        query_text: RequÃªte texte (franÃ§ais)
        index_path: Chemin vers l'index FAISS
        metadata_path: Chemin vers les mÃ©tadonnÃ©es
        top_k: Nombre de rÃ©sultats Ã  retourner
        model_name: Nom du modÃ¨le CLIP
        embedder: Instance de CLIPEmbedder (optionnel, sera crÃ©Ã© si None)
        use_query_expansion: Si True, utilise la query expansion bilingue
        auto_translate: Si True, traduit automatiquement la requÃªte en anglais
        use_dynamic_threshold: Si True, utilise un seuil dynamique
        fixed_threshold: Seuil fixe si use_dynamic_threshold=False
        always_rerank: Si True, applique toujours le rerank
        rerank_if_below: Si non-None et best_cosine < rerank_if_below, applique rerank
        use_reranking: Si True, active le rerank (dÃ©prÃ©ciÃ©, utiliser always_rerank ou rerank_if_below)
        use_captions: Si True, utilise les captions pour le rerank
        
    Returns:
        Liste de dictionnaires avec "path", "score", "cosine_score", "meta"
    """
    # Charger l'index et les mÃ©tadonnÃ©es
    index, metadata = load_index_and_metadata(index_path, metadata_path)
    
    # Initialiser l'embedder si nÃ©cessaire
    if embedder is None:
        try:
            from .clip_utils import get_embedder
            embedder = get_embedder(model_name=model_name)
        except Exception as e:
            print(f"âŒ Erreur lors du chargement du modÃ¨le: {e}")
            raise
    
    # CompatibilitÃ© avec ancienne API
    if use_reranking and not always_rerank and rerank_if_below is None:
        always_rerank = True
    
    # Effectuer la recherche
    results = search(
        query_text=query_text,
        index=index,
        metadata=metadata,
        embedder=embedder,
        top_k=top_k,
        use_query_expansion=use_query_expansion,
        auto_translate=auto_translate,
        use_dynamic_threshold=use_dynamic_threshold,
        fixed_threshold=fixed_threshold,
        always_rerank=always_rerank,
        rerank_if_below=rerank_if_below,
        use_captions=use_captions
    )
    
    # VÃ©rifier si aucun rÃ©sultat aprÃ¨s filtrage
    if not results:
        print("\n" + "="*80)
        print("âš ï¸  AUCUN RÃ‰SULTAT PERTINENT")
        print("="*80)
        print(f"Aucun rÃ©sultat pertinent trouvÃ© pour cette requÃªte.")
        print("Essayez de reformuler votre requÃªte ou de rÃ©duire le seuil de similaritÃ©.")
        print("="*80 + "\n")
    else:
        # Afficher les rÃ©sultats
        display_results(results)
    
    return results
